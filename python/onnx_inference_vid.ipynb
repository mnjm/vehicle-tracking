{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir_path = Path(\"../input_videos\")\n",
    "out_dir_path = Path(\"./output_videos\")\n",
    "\n",
    "model_path = Path(\"../yolo11s.onnx\")\n",
    "iou_tresh = 0.45\n",
    "conf_tresh = 0.35\n",
    "label_d = {\n",
    "    1: \"bicycle\",\n",
    "    2: \"car\",\n",
    "    3: \"motorcycle\",\n",
    "    4: \"airplane\",\n",
    "    5: \"bus\",\n",
    "    6: \"train\",\n",
    "    7: \"truck\",\n",
    "    8: \"boat\",\n",
    "}\n",
    "imshow = True\n",
    "display_color = (0, 255, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectBBox:\n",
    "    def __init__(self, class_id, conf, center_x, center_y, width, height, scale_width=1, scale_height=1):\n",
    "        self.class_id = class_id\n",
    "        self.conf = conf\n",
    "        scaled_center = center_x * scale_width, center_y * scale_height\n",
    "        scaled_size = width * scale_width, height * scale_height\n",
    "        half_size = scaled_size[0] * 0.5, scaled_size[1] * 0.5\n",
    "        self.x1, self.y1 = scaled_center[0] - half_size[0], scaled_center[1] - half_size[1]\n",
    "        self.x2, self.y2 = scaled_center[0] + half_size[0], scaled_center[1] + half_size[1]\n",
    "        self.area = scaled_size[0] * scaled_size[1]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.class_id} ({self.conf}) {self.x1, self.y1} {self.x2, self.y2}\"\n",
    "\n",
    "def calc_iou(bbox1, bbox2):\n",
    "    x1 = max(bbox1.x1, bbox2.x1)\n",
    "    y1 = max(bbox1.y1, bbox2.y1)\n",
    "    x2 = min(bbox1.x2, bbox2.x2)\n",
    "    y2 = min(bbox1.y2, bbox2.y2)\n",
    "    inter_w, inter_h = x2 - x1, y2 - y1\n",
    "    if inter_w <= 0 or inter_h <= 0:\n",
    "        return 0\n",
    "    interArea = inter_w * inter_h\n",
    "    union = bbox1.area + bbox2.area - interArea\n",
    "    if union <= 0:\n",
    "        return 0\n",
    "    return interArea / union\n",
    "\n",
    "class YOLOv11:\n",
    "    \n",
    "    input_size = np.array((640, 640))\n",
    "    providers = [\"CUDAExecutionProvider\", \"CoreMLExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    \n",
    "    def __init__(self, model_path, min_conf, iou_thresh, valid_class_checker=None):\n",
    "        self.model_path = model_path\n",
    "        self.ort_sess = ort.InferenceSession(\n",
    "            model_path, providers=YOLOv11.providers\n",
    "        )\n",
    "        self.input_name = self.ort_sess.get_inputs()[0].name\n",
    "        self.output_name = self.ort_sess.get_outputs()[0].name\n",
    "        self.min_conf = min_conf\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.valid_class_checker = valid_class_checker if valid_class_checker else lambda cls: True\n",
    "            \n",
    "        ## Get class name map from onnx model's metadata\n",
    "        meta = self.ort_sess.get_modelmeta()\n",
    "        custom_metadata = meta.custom_metadata_map  # Corrected key name\n",
    "        assert \"names\" in custom_metadata, \"Error: ONNX model does not contain 'names' metadata\"\n",
    "        self.class_name_map = literal_eval(custom_metadata[\"names\"])\n",
    "\n",
    "    def _preprocess_input(self, input_img):\n",
    "        inp = cv2.resize(input_img, YOLOv11.input_size)\n",
    "        inp = inp.astype(np.float32) / 255.0\n",
    "        inp = np.transpose(inp, (2, 0, 1)) # from HxWxC to CxHxW\n",
    "        inp = np.expand_dims(inp, axis=0)\n",
    "        return inp\n",
    "    \n",
    "    # perform Non Max Suppression\n",
    "    def _postprocess_output(self, raw_out, original_shape):\n",
    "        raw_out = np.squeeze(raw_out)\n",
    "        assert raw_out.shape[0] == 4 + len(self.class_name_map), f\"Output not in valid shape {raw_out.shape}\"\n",
    "        n_det = raw_out.shape[1]\n",
    "        assert n_det > 0, f\"Output not in valid shape {raw_out.shape}\"\n",
    "        scale = np.array(original_shape) / YOLOv11.input_size\n",
    "        \n",
    "        # List to keep track of valid bboxs\n",
    "        valid_bbox_l = []\n",
    "        # Keep track of suppressed detections\n",
    "        suppresed_mask = np.zeros(n_det, dtype=bool)\n",
    "        \n",
    "        ## Sort object scores in descending order\n",
    "        # find out the max scored class in each detection\n",
    "        class_idx_l = np.argmax(raw_out[4:, :], axis=0)\n",
    "        # get max scores for each detection\n",
    "        max_conf_l = np.take_along_axis(raw_out[4:, :], class_idx_l[None, :], axis=0).squeeze()\n",
    "        # now sort based on scores\n",
    "        order_idx_l = np.argsort(max_conf_l)[::-1] # descending\n",
    "        \n",
    "        ## Iterate through all detections\n",
    "        for i, idx1 in enumerate(order_idx_l):\n",
    "            cls = class_idx_l[idx1]\n",
    "            conf = max_conf_l[idx1]\n",
    "            # if it is suppressed or if not a valid class or if conf is less then min_conf\n",
    "            if suppresed_mask[idx1] or (not self.valid_class_checker(cls)) or conf < self.min_conf:\n",
    "                continue\n",
    "            lbl = self.class_name_map[cls]\n",
    "            bbox1 = ObjectBBox(lbl, conf, *raw_out[:4, idx1], scale[1], scale[0])\n",
    "            ## Select detection as valid\n",
    "            valid_bbox_l.append(bbox1)\n",
    "            \n",
    "            for idx2 in order_idx_l[i+1:]:\n",
    "                cls = class_idx_l[idx2]\n",
    "                conf = max_conf_l[idx2]\n",
    "                # if it is suppressed or if not a valid class or if conf is less then min_conf\n",
    "                if suppresed_mask[idx2] or (not self.valid_class_checker(cls)) or conf < self.min_conf:\n",
    "                    continue\n",
    "                lbl = self.class_name_map[cls]\n",
    "                bbox2 = ObjectBBox(lbl, conf, *raw_out[:4, idx2], scale[1], scale[0])\n",
    "                iou = calc_iou(bbox1, bbox2)\n",
    "                if (iou > self.iou_thresh):\n",
    "                    suppresed_mask[idx2] = True\n",
    "        \n",
    "        return valid_bbox_l\n",
    "    \n",
    "    def detect(self, image):\n",
    "        image = np.squeeze(image)\n",
    "        assert isinstance(image, np.ndarray) and image.dtype == np.uint8, \"Not a valid image\"\n",
    "        shape = image.shape\n",
    "        assert len(shape) == 3 and shape[-1] == 3, \"Input is not in HxWxC formaat\"\n",
    "        original_shape = shape[:2]\n",
    "        \n",
    "        # Preprocess\n",
    "        inp = self._preprocess_input(image)\n",
    "        # Infer\n",
    "        out_l = self.ort_sess.run([self.output_name], {self.input_name: inp})\n",
    "        if len(out_l) != 1:\n",
    "            print(\"Error: Infering YOLOv11 failed.\")\n",
    "            return None\n",
    "        bbox_l = self._postprocess_output(out_l[0], original_shape)\n",
    "        return bbox_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnjm/.pyvenv/dl_default/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:115: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CoreMLExecutionProvider, AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2025-02-08 10:48:19.268221 [W:onnxruntime:, coreml_execution_provider.cc:115 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 15 number of nodes in the graph: 320 number of nodes supported by CoreML: 304\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = YOLOv11(\n",
    "    model_path=model_path,\n",
    "    min_conf=conf_tresh,\n",
    "    iou_thresh=iou_tresh,\n",
    "    valid_class_checker=lambda x: x in label_d.keys(),\n",
    ")\n",
    "\n",
    "# Input\n",
    "vid_path_l = list(vid_dir_path.glob(\"*.mp4\"))\n",
    "assert len(vid_path_l) > 0, \"Videos not found\"\n",
    "\n",
    "# Output\n",
    "out_dir_path.mkdir(exist_ok=True)\n",
    "assert out_dir_path.is_dir(), f\"Error in creating out dir {out_dir_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 10:48:22.369 Python[7956:134822] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-08 10:48:22.369 Python[7956:134822] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "for vid_path in vid_path_l:\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error in reading {vid_path} video\")\n",
    "        continue\n",
    "    out_vid_p = out_dir_path / vid_path.name\n",
    "    \n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(out_vid_p, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break \n",
    "        \n",
    "        bbox_l = model.detect(frame)\n",
    "        if imshow:\n",
    "            cv2.namedWindow(vid_path.name)\n",
    "        \n",
    "        for bbox in bbox_l:\n",
    "            label = bbox.class_id\n",
    "            conf = bbox.conf\n",
    "            x1, y1, x2, y2 = map(lambda x: int(x), (bbox.x1, bbox.y1, bbox.x2, bbox.y2))\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), display_color, 2) \n",
    "            cv2.putText(frame, f\"{label}({conf:.2f})\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, display_color, 2)\n",
    "        \n",
    "        if imshow:\n",
    "            cv2.imshow(vid_path.name, frame) \n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
